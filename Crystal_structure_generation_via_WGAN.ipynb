{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGp7PQL0xTQW"
      },
      "outputs": [],
      "source": [
        "import sklearn.model_selection\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import random\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LayerNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Layer\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from keras import Input\n",
        "from keras import Model\n",
        "from keras.utils import generic_utils\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from tensorflow.python.keras.layers.merge import _Merge\n",
        "from keras.layers import InputSpec\n",
        "import keras.backend as K\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ckyD7eibGke"
      },
      "outputs": [],
      "source": [
        "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Performs spectral normalization on weights.\n",
        "    This wrapper controls the Lipschitz constant of the layer by\n",
        "    constraining its spectral norm, which can stabilize the training of GANs.\n",
        "    See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer: tf.keras.layers, power_iterations: int = 1, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        if power_iterations <= 0:\n",
        "            raise ValueError(\n",
        "                \"`power_iterations` should be greater than zero, got \"\n",
        "                \"`power_iterations={}`\".format(power_iterations)\n",
        "            )\n",
        "        self.power_iterations = power_iterations\n",
        "        self._initialized = False\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build `Layer`\"\"\"\n",
        "        super().build(input_shape)\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\n",
        "\n",
        "        if hasattr(self.layer, \"kernel\"):\n",
        "            self.w = self.layer.kernel\n",
        "        elif hasattr(self.layer, \"embeddings\"):\n",
        "            self.w = self.layer.embeddings\n",
        "        else:\n",
        "            raise AttributeError(\n",
        "                \"{} object has no attribute 'kernel' nor \"\n",
        "                \"'embeddings'\".format(type(self.layer).__name__)\n",
        "            )\n",
        "\n",
        "        self.w_shape = self.w.shape.as_list()\n",
        "\n",
        "        self.u = self.add_weight(\n",
        "            shape=(1, self.w_shape[-1]),\n",
        "            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
        "            trainable=False,\n",
        "            name=\"sn_u\",\n",
        "            dtype=self.w.dtype,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"Call `Layer`\"\"\"\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "\n",
        "        if training:\n",
        "            self.normalize_weights()\n",
        "\n",
        "        output = self.layer(inputs)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
        "\n",
        "    @tf.function\n",
        "    def normalize_weights(self):\n",
        "        \"\"\"Generate spectral normalized weights.\n",
        "        This method will update the value of `self.w` with the\n",
        "        spectral normalized value, so that the layer is ready for `call()`.\n",
        "        \"\"\"\n",
        "\n",
        "        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
        "        u = self.u\n",
        "\n",
        "        with tf.name_scope(\"spectral_normalize\"):\n",
        "            for _ in range(self.power_iterations):\n",
        "                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n",
        "                u = tf.math.l2_normalize(tf.matmul(v, w))\n",
        "\n",
        "            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n",
        "\n",
        "            self.w.assign(self.w / sigma)\n",
        "            self.u.assign(u)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"power_iterations\": self.power_iterations}\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "\n",
        "    def __init__(self, ch, **kwargs):\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "        self.channels = ch\n",
        "        self.filters_f_g = self.channels // 8\n",
        "        self.filters_h = self.channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
        "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
        "\n",
        "        # Create a trainable weight variable for this layer:\n",
        "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
        "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_f',\n",
        "                                        trainable=True)\n",
        "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_g',\n",
        "                                        trainable=True)\n",
        "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_h',\n",
        "                                        trainable=True)\n",
        "\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=4,\n",
        "                                    axes={3: input_shape[-1]})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        def hw_flatten(x):\n",
        "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[3]])\n",
        "\n",
        "        f = K.conv2d(x,\n",
        "                     kernel=self.kernel_f,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
        "        g = K.conv2d(x,\n",
        "                     kernel=self.kernel_g,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
        "        h = K.conv2d(x,\n",
        "                     kernel=self.kernel_h,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
        "\n",
        "        s = K.batch_dot(hw_flatten(g), K.permute_dimensions(hw_flatten(f), (0, 2, 1)))  # # [bs, N, N]\n",
        "\n",
        "        beta = K.softmax(s, axis=-1)  # attention map\n",
        "\n",
        "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
        "\n",
        "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
        "        x = self.gamma * o + x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "  \n",
        "def input_shapes(model, prefix):\n",
        "    shapes = [il.shape[1:] for il in \n",
        "        model.inputs if il.name.startswith(prefix)]\n",
        "    shapes = [tuple([d for d in dims]) for dims in shapes]\n",
        "    return shapes\n",
        "\n",
        "class NoiseGenerator(object):\n",
        "    def __init__(self, noise_shapes, batch_size=512, random_seed=None):\n",
        "        self.noise_shapes = noise_shapes\n",
        "        self.batch_size = batch_size\n",
        "        self.prng = np.random.RandomState(seed=random_seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self, mean=0.0, std=1.0):\n",
        "\n",
        "        def noise(shape):\n",
        "            shape = (self.batch_size,) + shape\n",
        "\n",
        "            n = self.prng.randn(*shape).astype(np.float32)\n",
        "            if std != 1.0:\n",
        "                n *= std\n",
        "            if mean != 0.0:\n",
        "                n += mean\n",
        "            return n\n",
        "\n",
        "        return [noise(s) for s in self.noise_shapes]\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred, axis=-1)\n",
        "\n",
        "class RandomWeightedAverage(_Merge):\n",
        "    def build(self, input_shape):\n",
        "        super(RandomWeightedAverage, self).build(input_shape)\n",
        "        if len(input_shape) != 2:\n",
        "            raise ValueError('A `RandomWeightedAverage` layer should be '\n",
        "                             'called on exactly 2 inputs')\n",
        "\n",
        "    def _merge_function(self, inputs):\n",
        "        if len(inputs) != 2:\n",
        "            raise ValueError('A `RandomWeightedAverage` layer should be '\n",
        "                             'called on exactly 2 inputs')\n",
        "\n",
        "        (x,y) = inputs\n",
        "        shape = K.shape(x)\n",
        "        weights = K.random_uniform(shape[:1],0,1)\n",
        "        for i in range(len(K.int_shape(x))-1):\n",
        "            weights = K.expand_dims(weights,-1)\n",
        "        rw = x*weights + y*(1-weights)\n",
        "        return rw\n",
        "\n",
        "\n",
        "class Nontrainable(object):\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.trainable_status = self.model.trainable\n",
        "        self.model.trainable = False\n",
        "        return self.model\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.model.trainable = self.trainable_status\n",
        "\n",
        "class GradientPenalty(Layer):\n",
        "    def call(self, inputs):\n",
        "        real_image, generated_image, disc = inputs\n",
        "        avg_image = RandomWeightedAverage()(\n",
        "        [real_image, generated_image]\n",
        "        )\n",
        "        with tf.GradientTape() as tape:\n",
        "          tape.watch(avg_image)\n",
        "          disc_avg = disc(avg_image)\n",
        "        \n",
        "        grad = tape.gradient(disc_avg,[avg_image])[0]\n",
        "        print(grad)\n",
        "        GP = K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))-1\n",
        "        return GP\n",
        "############################## Normalizing real data ################################\n",
        "\n",
        "#source: https://stackoverflow.com/questions/58646790/python-scaling-with-4d-data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scaling (data):\n",
        "  X_transformed = np.zeros_like(data)\n",
        "  mmx = MinMaxScaler()\n",
        "  slc = data[:, :, :, 0].reshape(23951, 238*62) # make it a bunch of row vectors\n",
        "  transformed = mmx.fit_transform(slc)\n",
        "  transformed = transformed.reshape(23951, 238, 62) # reshape it back to tiles\n",
        "  X_transformed[:, :, :, 0] = transformed # put it in the transformed array\n",
        "  return X_transformed\n",
        "\n",
        "\n",
        "def compute_output_shape(self, input_shapes):\n",
        "        return (input_shapes[1][0], 1)\n",
        "\n",
        "def load_real_samples():\n",
        "    trainX = np.load(\"/content/drive/MyDrive/FTCP_rep.npy\",allow_pickle = True)\n",
        "    trainX = expand_dims(trainX,axis = -1)\n",
        "    X = trainX.astype('float32')\n",
        "    X = scaling(X)\n",
        "    return X\n",
        "\n",
        "X_train = load_real_samples() \n",
        "input_dim = X_train.shape[1]\n",
        "channel_dim = X_train.shape[2]\n",
        "\n",
        "def generate_real_samples(dataset,n_samples):\n",
        "    ix = randint(0,dataset.shape[0],n_samples)\n",
        "    X = dataset[ix]\n",
        "    y = -ones((n_samples,1))\n",
        "    return X,y\n",
        "    \n",
        "def generate_latent_points(latent_dim,n_samples):\n",
        "    x_input = randn(latent_dim*n_samples)\n",
        "    x_input = x_input.reshape(n_samples,latent_dim)\n",
        "    return x_input\n",
        "\n",
        "def generate_fake_samples(generator,latent_dim,n_samples):\n",
        "    x_input = generate_latent_points(latent_dim,n_samples)\n",
        "    X = generator.predict(x_input)\n",
        "    y = ones((n_samples,1))\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uk77ZayyTbi"
      },
      "outputs": [],
      "source": [
        "############################### Defining discriminator function ###############################\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Lambda, Conv1D, Conv2DTranspose, \\\n",
        "    LeakyReLU, Activation, Flatten, Reshape, BatchNormalization\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "latent_dim = 512\n",
        "max_filters = 128\n",
        "filter_size = [5,3,3]\n",
        "strides = [2,2,1]\n",
        "\n",
        "def define_critic(in_shape):\n",
        "  max_filters = 128\n",
        "  filter_size = [5,3,3]\n",
        "  strides = [2,2,1]\n",
        "  critic_inputs = Input(shape=(input_dim, channel_dim,), name=\"input\")\n",
        "  x = SpectralNormalization(Conv1D(max_filters//4, filter_size[0], strides=strides[0], padding='SAME'))(critic_inputs) #Specteral normalization added.\n",
        "  x = LayerNormalization()(x) #Newly added\n",
        "  x = LeakyReLU(0.2)(x)\n",
        "  \n",
        "  x = SpectralNormalization(Conv1D(max_filters//2, filter_size[1], strides=strides[1], padding='SAME'))(x) #Specteral normalization added.\n",
        "  x = LayerNormalization()(x) #Newly added\n",
        "  x = LeakyReLU(0.2)(x)\n",
        "\n",
        "  x = SpectralNormalization(Conv1D(max_filters, filter_size[2], strides=strides[2], padding='SAME'))(x) #Specteral normalization added.\n",
        "  x = LayerNormalization()(x) #Newly added\n",
        "  x = LeakyReLU(0.2)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation=\"relu\")(x)\n",
        "  out = Dense(1, activation = 'linear')(x)\n",
        "  model = Model(inputs=critic_inputs, outputs=out)\n",
        "  for layer in model.layers:\n",
        "    print(layer.output_shape)\n",
        "  return model\n",
        "\n",
        "############################### Defining generator function ###############################\n",
        "\n",
        "def generate_real_samples(dataset,n_samples):\n",
        "    ix = randint(0,dataset.shape[0],n_samples)\n",
        "    X = dataset[ix]\n",
        "    y = -ones((n_samples,1))\n",
        "    return X,y\n",
        "    \n",
        "def generate_latent_points(latent_dim,n_samples):\n",
        "    x_input = randn(latent_dim*n_samples)\n",
        "    x_input = x_input.reshape(n_samples,latent_dim)\n",
        "    return x_input\n",
        "    \n",
        "noise = generate_latent_points(512,10)\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "    latent_inputs = Input(shape=(latent_dim,), name=\"noise_input\")\n",
        "    map_size =  59  \n",
        "\n",
        "    x = SpectralNormalization(Dense(max_filters*map_size, activation='LeakyReLU'))(latent_inputs) #Specteral normalization added.\n",
        "    x = LayerNormalization()(x) #Newly added\n",
        "    x = Reshape((map_size, 1, max_filters))(x) \n",
        "    #x = BatchNormalization()(x)\n",
        "    print(x.shape)\n",
        "    x = SpectralNormalization(Conv2DTranspose(max_filters//2, (3, 1), strides=(strides[2], 1), \n",
        "                        padding='SAME'))(x) #Specteral normalization added.\n",
        "    x = LayerNormalization()(x) #Newly added\n",
        "\n",
        "    print(x.shape)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('LeakyReLU')(x)\n",
        "    x = SpectralNormalization(Conv2DTranspose(max_filters//4, (3, 1), strides=(strides[1], 1), \n",
        "                        padding='VALID'))(x) #Specteral normalization added.\n",
        "    x = LayerNormalization()(x) #Newly added                    \n",
        "    print(\"x\",x.shape) \n",
        "   # x = BatchNormalization()(x)\n",
        "    x = Activation('LeakyReLU')(x)\n",
        "    x = SpectralNormalization(Conv2DTranspose(channel_dim, (3,1), strides=(2,1), \n",
        "                        padding='SAME'))(x) #Specteral normalization added.\n",
        "    x = LayerNormalization()(x) #Newly added                    \n",
        "    print(x.shape)\n",
        "    x = Activation('sigmoid')(x) #sigmoid is returned. In Second GAN, it was tanh.\n",
        "    out = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
        "    model = Model(inputs=latent_inputs, outputs=out)\n",
        "    for layer in model.layers:\n",
        "      print(layer.output_shape)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuT3A-ISIf-D"
      },
      "outputs": [],
      "source": [
        "class WGANGP(object):\n",
        "    def __init__(self, gen, disc, lr_gen=0.0001, lr_disc=0.0001):\n",
        "\n",
        "      self.gen = gen\n",
        "      self.disc = disc\n",
        "      self.lr_gen = lr_gen\n",
        "      self.lr_disc = lr_disc\n",
        "      self.build()\n",
        "\n",
        "    def build(self):\n",
        "        \n",
        "        tens_shape = input_shapes(self.disc, \"input\")[0]\n",
        "        noise_shapes = input_shapes(self.gen, \"noise_input\")\n",
        "\n",
        "        self.opt_disc = Adam(self.lr_disc, beta_1=0.0, beta_2=0.9)\n",
        "        self.opt_gen = Adam(self.lr_gen, beta_1=0.0, beta_2=0.9)\n",
        "        \n",
        "        with Nontrainable(self.gen):\n",
        "            real_image = Input(shape=tens_shape)\n",
        "            noise = [Input(shape=s) for s in noise_shapes]\n",
        "            \n",
        "            print(\"real_image\", real_image.shape)\n",
        "            disc_real = self.disc(real_image)\n",
        "            print(\"disc_real\", disc_real.shape)\n",
        "            generated_image = self.gen(noise)\n",
        "            print(\"generated_image\", generated_image.shape)\n",
        "            disc_fake = self.disc(generated_image)\n",
        "\n",
        "\n",
        "            gp = GradientPenalty()([real_image, generated_image, self.disc])\n",
        "\n",
        "            self.disc_trainer = Model(\n",
        "                inputs=[real_image, noise],\n",
        "                outputs=[disc_real, disc_fake, gp]\n",
        "            )\n",
        "            self.disc_trainer.compile(optimizer=self.opt_disc,\n",
        "                loss=[wasserstein_loss, wasserstein_loss, 'mse'],\n",
        "                loss_weights=[1.0, 1.0, 10.0]\n",
        "            )\n",
        "        \n",
        "        with Nontrainable(self.disc):\n",
        "            noise = [Input(shape=s) for s in noise_shapes]\n",
        "            \n",
        "            generated_image = self.gen(noise)\n",
        "            disc_fake = self.disc(generated_image)\n",
        "            \n",
        "            self.gen_trainer = Model(\n",
        "                inputs=noise, \n",
        "                outputs=disc_fake\n",
        "            )\n",
        "            self.gen_trainer.compile(optimizer=self.opt_gen,\n",
        "                loss=wasserstein_loss)\n",
        "      \n",
        "    def fit_generator(self, noise_gen, dataset, latent_dim, n_epochs = 10, n_batch = 256,n_critic = 5):\n",
        "        bat_per_epoch = int(dataset.shape[0]/n_batch)\n",
        "        n_steps = bat_per_epoch*n_epochs\n",
        "        half_batch = int(n_batch/2)\n",
        "        disc_out_shape = (n_batch, self.disc.output_shape[1])\n",
        "        real_target = -np.ones(disc_out_shape, dtype=np.float32)\n",
        "        fake_target = -real_target\n",
        "        gp_target = np.ones_like(real_target)\n",
        "        genLossArr = []\n",
        "        disc0LossArr = []\n",
        "        disc1LossArr = []\n",
        "        disc2LossArr = []\n",
        "        disc3LossArr = []\n",
        "        for epoch in range(n_epochs):\n",
        "            print(\"Epoch {}/{}\".format(epoch+1, n_epochs))\n",
        "            # Initialize progbar and batch counter\n",
        "            progbar = generic_utils.Progbar(\n",
        "                bat_per_epoch*n_batch)\n",
        "            for step in range(bat_per_epoch):\n",
        "\n",
        "                # Train discriminator\n",
        "                with Nontrainable(self.gen):\n",
        "                    for repeat in range(n_critic):\n",
        "                        tens_batch = generate_real_samples(dataset, n_batch)\n",
        "                        noise_batch = next(noise_gen)\n",
        "                        disc_loss = self.disc_trainer.train_on_batch(\n",
        "                            [tens_batch[0]]+noise_batch,\n",
        "                            [real_target, fake_target, gp_target]\n",
        "                        )\n",
        "\n",
        "                # Train generator\n",
        "                with Nontrainable(self.disc):\n",
        "                    noise_batch = next(noise_gen)\n",
        "                    gen_loss = self.gen_trainer.train_on_batch(\n",
        "                        noise_batch, real_target)\n",
        "                losses = []\n",
        "\n",
        "                for (i,dl) in enumerate(disc_loss):\n",
        "                    losses.append((\"D{}\".format(i), dl))\n",
        "                    if i == 0:\n",
        "                      disc0LossArr.append(dl)\n",
        "                    elif i == 1:\n",
        "                      disc1LossArr.append(dl)\n",
        "                    elif i == 2:\n",
        "                      disc2LossArr.append(dl)\n",
        "                    elif i == 3:\n",
        "                      disc3LossArr.append(dl)\n",
        "                losses.append((\"G0\", gen_loss))\n",
        "                genLossArr.append(gen_loss)\n",
        "                progbar.add(n_batch, \n",
        "                    values=losses)\n",
        "            self.disc.save(\"/content/drive/MyDrive/WGAN_2/WGANGPDisc_1\")\n",
        "            self.gen.save(\"/content/drive/MyDrive/WGAN_2/WGANGPNorm_1\")\n",
        "            np.savetxt(\"/content/drive/MyDrive/WGAN_2/WGPDNorm0Loss_1\",disc0LossArr)\n",
        "            np.savetxt(\"/content/drive/MyDrive/WGAN_2/WGPDNorm1Loss_1\",disc1LossArr)\n",
        "            np.savetxt(\"/content/drive/MyDrive/WGAN_2/WGPDNorm2Loss_1\",disc2LossArr)\n",
        "            np.savetxt(\"/content/drive/MyDrive/WGAN_2/WGPDNorm3Loss_1\",disc3LossArr)\n",
        "            np.savetxt(\"/content/drive/MyDrive/WGAN_2/WGPNormGloss_1\",genLossArr)\n",
        "\n",
        "noise_dim = 512\n",
        "critic = define_critic(7)\n",
        "generator = define_generator(noise_dim)\n",
        "gan = WGANGP(generator, critic)\n",
        "dataset = load_real_samples()\n",
        "noise_gen = NoiseGenerator([(noise_dim,)])\n",
        "gan.fit_generator(noise_gen, dataset, noise_dim, n_epochs = 100, n_batch = 512, n_critic = 5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Crystal structure generation via WGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}